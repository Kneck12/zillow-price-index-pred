{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85525141",
   "metadata": {
    "id": "85525141"
   },
   "source": [
    "### Goal: Try again on estimation on region\n",
    "Based on all the features we get this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348d282",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3499,
     "status": "ok",
     "timestamp": 1632308565777,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "6348d282",
    "outputId": "d14f1595-b4ff-4b3b-8215-67c3650a6f83"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt;\n",
    "print(tf.__version__);\n",
    "import pandas as pd;\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "from sklearn.preprocessing import MinMaxScaler;\n",
    "from sklearn.metrics import mean_squared_error;\n",
    "\n",
    "import datetime;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132082d",
   "metadata": {
    "id": "2132082d"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 300);\n",
    "pd.set_option('display.max_rows', 300);\n",
    "tf.keras.backend.set_floatx('float64');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c2830",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1632308566278,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "b69c2830",
    "outputId": "237f55ab-10f6-4a54-8cde-4d8a8f23534d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d643a9",
   "metadata": {
    "id": "66d643a9"
   },
   "outputs": [],
   "source": [
    "MONTHS = 60;\n",
    "SPLIT = 48; # 2015-2018: training, 2019: testing.\n",
    "# BATCH_SIZE = 19; # used in NN_v1\n",
    "BATCH_SIZE = 24;\n",
    "WINDOW_SIZE = 3;\n",
    "\n",
    "TEST_LENGTH = MONTHS - SPLIT;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29c1cf",
   "metadata": {
    "id": "9c29c1cf"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b8aac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "executionInfo": {
     "elapsed": 25630,
     "status": "ok",
     "timestamp": 1632308592088,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "763b8aac",
    "outputId": "12b20483-360a-4e34-8a1b-273d1bdaf228",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_data = pd.read_csv('../data/full_dataset_engineered.csv', index_col=0);\n",
    "zip_ids = multi_data.index.unique();\n",
    "\n",
    "multi_data.drop([\"City\", \"State\", \"Metro\", \"CountyName\", \"year\", \"month\", \"year-month\"],\\\n",
    "                 axis = 1, inplace = True);\n",
    "\n",
    "multi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d05f0",
   "metadata": {
    "id": "2a9d05f0"
   },
   "outputs": [],
   "source": [
    "FEATURES = multi_data.shape[1] - 1;\n",
    "\n",
    "feature_name = list(multi_data.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fac97c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1632308592090,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "84fac97c",
    "outputId": "136ccdee-bb1e-479e-820a-9dcdd44787f8"
   },
   "outputs": [],
   "source": [
    "# In our first try, just look at the zip codes in NY. zip:10001-14905\n",
    "multi_zip = zip_ids;\n",
    "# list(multi_data[(multi_data.index >= 10001) & (multi_data.index <= 14905)].index.unique());\n",
    "print(len(multi_zip))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b815f",
   "metadata": {
    "id": "554b815f"
   },
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839569c0",
   "metadata": {
    "id": "839569c0"
   },
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series); #(43,)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True);\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1)); #(13,43)\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\\\n",
    "                     .map(lambda window: (window[:-1], window[-1][0]));\n",
    "    dataset = dataset.batch(batch_size);\n",
    "    return dataset;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5dcf3b",
   "metadata": {
    "id": "9c5dcf3b"
   },
   "outputs": [],
   "source": [
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time Frame\")\n",
    "    plt.ylabel(\"ZRI\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad66377",
   "metadata": {
    "id": "6ad66377"
   },
   "source": [
    "### Neural network center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eaf0e5",
   "metadata": {
    "id": "d9eaf0e5"
   },
   "outputs": [],
   "source": [
    "def NN_dataprep(multi):\n",
    "    cities_stats = {};\n",
    "    dataset = windowed_dataset(np.zeros((1, FEATURES+1)), WINDOW_SIZE, BATCH_SIZE, 60);\n",
    "    validset = windowed_dataset(np.zeros((1, FEATURES+1)), WINDOW_SIZE, BATCH_SIZE, 60);\n",
    "\n",
    "    #for zip_num in multi_data[\"zip\"].unique():\n",
    "    for zip_num in multi:\n",
    "        test = multi_data[multi_data.index == zip_num];\n",
    "        single_city_series = np.array(test);\n",
    "\n",
    "        scaler = MinMaxScaler();\n",
    "        \n",
    "        single_city_transformed = scaler.fit_transform(single_city_series)\n",
    "        \n",
    "        cities_stats[zip_num] = {\"min\": scaler.data_min_[0],\n",
    "                                 \"range\": scaler.data_range_[0]};\n",
    "\n",
    "        single_city_train = single_city_transformed[:SPLIT];\n",
    "        single_city_test = single_city_transformed[SPLIT-WINDOW_SIZE:];\n",
    "\n",
    "        cityset = windowed_dataset(single_city_train, WINDOW_SIZE, BATCH_SIZE, 60);\n",
    "        cityvalid = windowed_dataset(single_city_test, WINDOW_SIZE, BATCH_SIZE, 60);\n",
    "\n",
    "        dataset = dataset.concatenate(cityset);\n",
    "        validset = validset.concatenate(cityvalid);\n",
    "        \n",
    "    dataset = dataset.prefetch(1);\n",
    "    validset = validset.prefetch(1);\n",
    "        \n",
    "    return dataset, validset, cities_stats;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679c1ee",
   "metadata": {
    "id": "c679c1ee"
   },
   "outputs": [],
   "source": [
    "def NN_model(dataset, termination=0, test=None):\n",
    "    tf.keras.backend.clear_session();\n",
    "    # dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "    class myCallbacks(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            mse = logs.get(\"mse\");\n",
    "            if(mse < termination):\n",
    "                print(\"\\nGot an mse at {:.4f} in epoch {} and stopped training\\n\".format(mse, epoch));\n",
    "                self.model.stop_training = True;\n",
    "            \n",
    "    callback = myCallbacks();\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
    "#                           input_shape=[None]),\n",
    "#       tf.keras.layers.Conv1D(filters=32, kernel_size=3,\n",
    "#                           strides=1, padding=\"causal\",\n",
    "#                           activation=\"relu\",\n",
    "#                           input_shape=[None, WINDOW_SIZE, FEATURES+1]),\n",
    "        tf.keras.layers.LSTM(32, return_sequences=False,\n",
    "                             input_shape = [WINDOW_SIZE, FEATURES+1]),\n",
    "        #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n",
    "        # tf.keras.layers.SimpleRNN(8, return_sequences=True),\n",
    "        #  tf.keras.layers.SimpleRNN(16, return_sequences=True),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "    #   tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1)\n",
    "        # tf.keras.layers.Lambda(lambda x: x * 2.0)\n",
    "    ]);\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=3e-4, momentum=0.9)\n",
    "    model.compile(loss=tf.keras.losses.Huber(),\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"mae\", \"mse\"])\n",
    "\n",
    "    model.build((None,WINDOW_SIZE,FEATURES+1))\n",
    "    model.summary()\n",
    "\n",
    "    if not test: history = model.fit(dataset, epochs=20, callbacks = [callback], verbose = 1);\n",
    "    else: history = model.fit(dataset, epochs=20, validation_data=test,\\\n",
    "                              callbacks=[callback], verbose = 1);\n",
    "    return model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a84b3",
   "metadata": {
    "id": "017a84b3"
   },
   "outputs": [],
   "source": [
    "def NN_forecast(model, series_transformed):\n",
    "    forecast = []\n",
    "    results = []\n",
    "    for time in range(MONTHS - WINDOW_SIZE):\n",
    "        forecast.append(model.predict(series_transformed[np.newaxis, time:time + WINDOW_SIZE, :]))\n",
    "\n",
    "    results = [float(x[-1][0]) for x in forecast];\n",
    "    timeline = range(WINDOW_SIZE, MONTHS);\n",
    "    time_test = range(SPLIT, MONTHS);\n",
    "    actual = list(series_transformed[WINDOW_SIZE:, 0]);\n",
    "\n",
    "    forecast = series_transformed[SPLIT - WINDOW_SIZE:,:].copy();\n",
    "\n",
    "    for time in range(TEST_LENGTH): # Change temp[time + WINDOW_SIZE]\n",
    "        forecast[time + WINDOW_SIZE, 0] =\\\n",
    "        model.predict(forecast[np.newaxis, time:time + WINDOW_SIZE, :])[-1][0];\n",
    "\n",
    "    pure_forecast = forecast[WINDOW_SIZE:,0];\n",
    "    \n",
    "    print(len(results), len(actual), len(pure_forecast))\n",
    "    \n",
    "    return results, actual, pure_forecast;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0879d68c",
   "metadata": {
    "id": "0879d68c"
   },
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def NN_group_test(multi, termination=0, plot=False):\n",
    "    '''\n",
    "    Input: A list of zip codes\n",
    "    Output: the RMSE of a NN model on the predicted train, partially predicted test, and complete predicted test.\n",
    "    '''\n",
    "    # Collection of data\n",
    "    dataset_train, dataset_test, city_stats = NN_dataprep(multi);\n",
    "        \n",
    "#     for x, y in dataset_train:\n",
    "#         print(np.array(x).shape, np.array(y).shape);\n",
    "\n",
    "    model = NN_model(dataset_train, termination, dataset_test);\n",
    "    model.save('.//saved_models/NN_model_{0:%Y-%m-%d %H-%M-%S}.h5'.format(datetime.datetime.now()));\n",
    "    \n",
    "    score_dict = {\"zip\":[], \"RMSE_train\":[], \"RMSE_test\":[], \"RMSE_pure\":[], \"forecast\":[]};\n",
    "\n",
    "    time_train = list(range(SPLIT));\n",
    "    time_test = list(range(SPLIT, MONTHS));\n",
    "    time_actual = range(WINDOW_SIZE, MONTHS);\n",
    "    \n",
    "    for zip_num in multi:\n",
    "        scaler = MinMaxScaler();\n",
    "        series = np.array(multi_data[multi_data.index == zip_num]);\n",
    "        series_transformed = scaler.fit_transform(series);\n",
    "    \n",
    "        # Forecasting\n",
    "        results, actual, pure_forecast = NN_forecast(model, series_transformed);\n",
    "\n",
    "        # Compute MSE\n",
    "        M_train = mean_squared_error(actual[:-TEST_LENGTH], results[:-TEST_LENGTH])**0.5 * scaler.data_range_[0];\n",
    "        M_test = mean_squared_error(actual[-TEST_LENGTH:], results[-TEST_LENGTH:])**0.5 * scaler.data_range_[0];\n",
    "        M_pure = mean_squared_error(actual[-TEST_LENGTH:], pure_forecast[-TEST_LENGTH:])**0.5 * scaler.data_range_[0];\n",
    "    \n",
    "        if plot: # If the plot option is selected, plot the graph.\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plot_series(time_actual, np.array(actual)*scaler.data_range_[0]+scaler.data_min_[0]);\n",
    "            plot_series(time_actual, np.array(results)*scaler.data_range_[0]+scaler.data_min_[0]);\n",
    "            plot_series(time_test, np.array(pure_forecast)*scaler.data_range_[0]+scaler.data_min_[0]);\n",
    "            plt.show();\n",
    "\n",
    "        score_dict[\"zip\"].append(zip_num);\n",
    "        score_dict[\"RMSE_train\"].append(M_train);\n",
    "        score_dict[\"RMSE_test\"].append(M_test);\n",
    "        score_dict[\"RMSE_pure\"].append(M_pure);\n",
    "        score_dict[\"forecast\"].append(pure_forecast);\n",
    "        print(zip_num, M_train, M_test, M_pure)\n",
    "\n",
    "    return score_dict;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff307a8",
   "metadata": {
    "id": "fff307a8"
   },
   "source": [
    "### The script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uxsf_0qB0HRB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "14X10mX8ceVe7PzfMzcSOeDbXDTML8KSU"
    },
    "executionInfo": {
     "elapsed": 11206771,
     "status": "ok",
     "timestamp": 1632328125769,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "uxsf_0qB0HRB",
    "outputId": "c9ecb291-fcfb-4dbd-b0c3-26cd232e6036"
   },
   "outputs": [],
   "source": [
    "score_dict = NN_group_test(multi_zip, 0.004, True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "io9wUbyX4ie9",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1632328125964,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "io9wUbyX4ie9"
   },
   "outputs": [],
   "source": [
    "score_dict;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4f8af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1632328125965,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "99d4f8af",
    "outputId": "35e12c17-94ca-42f3-c8e2-ab76aa8abb2e"
   },
   "outputs": [],
   "source": [
    "print(sum(score_dict[\"RMSE_test\"])/len(score_dict[\"RMSE_test\"]), sum(score_dict[\"RMSE_pure\"])/len(score_dict[\"RMSE_pure\"]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4RN9dWGuMHx1",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1632328125965,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "4RN9dWGuMHx1"
   },
   "outputs": [],
   "source": [
    "temp = score_dict.copy();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C7JuDUlnNltE",
   "metadata": {
    "executionInfo": {
     "elapsed": 3025,
     "status": "ok",
     "timestamp": 1632328128986,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "C7JuDUlnNltE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score;\n",
    "r2 = [];\n",
    "actual = [];\n",
    "predicted = [];\n",
    "for i in range(len(multi_zip)):\n",
    "    scaler = MinMaxScaler();\n",
    "    frame = np.array(multi_data[multi_data.index == multi_zip[i]][[\"zri\"]]);\n",
    "    frame = scaler.fit_transform(frame);\n",
    "\n",
    "    actual.extend(list(np.array(multi_data[multi_data.index == temp['zip'][i]][\"zri\"])[-TEST_LENGTH:]));\n",
    "    predicted.extend(list(temp['forecast'][i]*scaler.data_range_[0]+scaler.data_min_[0]));\n",
    "\n",
    "    # r2_zip = mean_squared_error(np.array(multi_data[multi_data.index == temp['zip'][i]][\"zri\"])[-TEST_LENGTH:], temp['forecast'][i]*scaler.data_range_[0]+scaler.data_min_[0]);\n",
    "    # print(r2_zip, np.array(multi_data[multi_data.index == temp['zip'][i]][\"zri\"])[-TEST_LENGTH:], temp['forecast'][i]*scaler.data_range_[0]+scaler.data_min_[0]);\n",
    "    # r2.append(r2_zip);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U8IKYFckWdDR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1632328128991,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "U8IKYFckWdDR",
    "outputId": "a60b9b18-176b-488d-a9b7-99e00467a015"
   },
   "outputs": [],
   "source": [
    "r2_score(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c431b",
   "metadata": {
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1632328129458,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "364c431b"
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(score_dict);\n",
    "test.to_csv('.//Results in CSV files/ZRI_and_engineered.csv', index = False);"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "_Tensorflow_region_zip_all_features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
