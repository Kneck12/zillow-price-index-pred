{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85525141",
   "metadata": {
    "id": "85525141"
   },
   "source": [
    "### Goal: Can we do better?\n",
    "- An issue on the prediction is, the ZRI value is probably not martingale for any time series for a fixed zip code, since there are inflation.\n",
    "- The idea is that we can try to differentiate the time series, that is, we can try to estimate $Z_i - Z_{i-1}$ instead of $Z_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348d282",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3499,
     "status": "ok",
     "timestamp": 1632308565777,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "6348d282",
    "outputId": "d14f1595-b4ff-4b3b-8215-67c3650a6f83"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt;\n",
    "print(tf.__version__);\n",
    "import pandas as pd;\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "from sklearn.preprocessing import MinMaxScaler;\n",
    "from sklearn.metrics import mean_squared_error;\n",
    "\n",
    "import datetime;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132082d",
   "metadata": {
    "id": "2132082d"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 300);\n",
    "pd.set_option('display.max_rows', 300);\n",
    "tf.keras.backend.set_floatx('float64');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c2830",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1632308566278,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "b69c2830",
    "outputId": "237f55ab-10f6-4a54-8cde-4d8a8f23534d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d643a9",
   "metadata": {
    "id": "66d643a9"
   },
   "outputs": [],
   "source": [
    "MONTHS = 60;\n",
    "SPLIT = 48; # 2015-2018: training, 2019: testing.\n",
    "# BATCH_SIZE = 19; # used in NN_v1\n",
    "BATCH_SIZE = 24;\n",
    "WINDOW_SIZE = 3;\n",
    "\n",
    "TEST_LENGTH = MONTHS - SPLIT;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29c1cf",
   "metadata": {
    "id": "9c29c1cf"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_data = pd.read_csv('../data/full_dataset_engineered.csv', index_col=0);\n",
    "zip_ids = multi_data.index.unique();\n",
    "\n",
    "multi_data.drop([\"City\", \"State\", \"Metro\", \"CountyName\", \"year\", \"month\", \"year-month\"],\\\n",
    "                 axis = 1, inplace = True);\n",
    "\n",
    "multi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b8aac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "executionInfo": {
     "elapsed": 25630,
     "status": "ok",
     "timestamp": 1632308592088,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "763b8aac",
    "outputId": "12b20483-360a-4e34-8a1b-273d1bdaf228",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive \n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# multi_data = pd.read_csv('gdrive/My Drive/full_dataset_unscaled.csv', index_col=0);\n",
    "# zip_ids = multi_data.index.unique();\n",
    "\n",
    "# multi_data.drop([\"City\", \"State\", \"Metro\", \"CountyName\", \"year\", \"month\", \"datetime\"],\\\n",
    "#                  axis = 1, inplace = True);\n",
    "\n",
    "# multi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d05f0",
   "metadata": {
    "id": "2a9d05f0"
   },
   "outputs": [],
   "source": [
    "FEATURES = multi_data.shape[1] - 1;\n",
    "\n",
    "feature_name = list(multi_data.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fac97c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1632308592090,
     "user": {
      "displayName": "Hw Chu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00746083124622974374"
     },
     "user_tz": 240
    },
    "id": "84fac97c",
    "outputId": "136ccdee-bb1e-479e-820a-9dcdd44787f8"
   },
   "outputs": [],
   "source": [
    "# In our first try, just look at the zip codes in NY. zip:10001-14905\n",
    "multi_zip = zip_ids;\n",
    "# list(multi_data[(multi_data.index >= 10001) & (multi_data.index <= 14905)].index.unique());\n",
    "print(len(multi_zip))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b815f",
   "metadata": {
    "id": "554b815f"
   },
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839569c0",
   "metadata": {
    "id": "839569c0"
   },
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series); #(43,)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True);\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1)); #(13,43)\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\\\n",
    "                     .map(lambda window: (window[:-1], window[-1][0]));\n",
    "    dataset = dataset.batch(batch_size);\n",
    "    return dataset;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5dcf3b",
   "metadata": {
    "id": "9c5dcf3b"
   },
   "outputs": [],
   "source": [
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time Frame\")\n",
    "    plt.ylabel(\"ZRI\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad66377",
   "metadata": {
    "id": "6ad66377"
   },
   "source": [
    "### Neural network center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a84b3",
   "metadata": {
    "id": "017a84b3"
   },
   "outputs": [],
   "source": [
    "def NN_forecast(model, series_transformed, pure=True):\n",
    "    forecast = []\n",
    "    results = []\n",
    "    for time in range(MONTHS - WINDOW_SIZE):\n",
    "        forecast.append(model.predict(series_transformed[np.newaxis, time:time + WINDOW_SIZE, :]))\n",
    "\n",
    "    results = [float(x[-1][0]) for x in forecast];\n",
    "    actual = list(series_transformed[WINDOW_SIZE:, 0]);\n",
    "    \n",
    "    if not pure:\n",
    "        return results, actual;\n",
    "\n",
    "    timeline = range(WINDOW_SIZE, MONTHS);\n",
    "    time_test = range(SPLIT, MONTHS);\n",
    "    forecast = series_transformed[SPLIT - WINDOW_SIZE:,:].copy();\n",
    "\n",
    "    for time in range(TEST_LENGTH): # Change temp[time + WINDOW_SIZE]\n",
    "        forecast[time + WINDOW_SIZE, 0] =\\\n",
    "        model.predict(forecast[np.newaxis, time:time + WINDOW_SIZE, :])[-1][0];\n",
    "\n",
    "    pure_forecast = forecast[WINDOW_SIZE:,0];\n",
    "    \n",
    "    print(len(results), len(actual), len(pure_forecast))\n",
    "    \n",
    "    return results, actual, pure_forecast;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e0551",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ebb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./saved_models/ZRI_and_all.h5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_importance = [];\n",
    "\n",
    "for zip_num in multi_zip:\n",
    "    scaler = MinMaxScaler();\n",
    "    series = np.array(multi_data[multi_data.index == zip_num]);\n",
    "    series_transformed = scaler.fit_transform(series);\n",
    "\n",
    "    # Forecasting\n",
    "    results, actual = NN_forecast(model, series_transformed, pure=False);\n",
    "\n",
    "    # Compute MSE\n",
    "    mse = mean_squared_error(actual[:-TEST_LENGTH], results[:-TEST_LENGTH])**0.5;\n",
    "    # Usually we need to multiply by scaler.data_range_[0], but can ignore this.\n",
    "    TRIALS = 1;\n",
    "    PLOT_IMPORTANCE = True;\n",
    "    \n",
    "    perm_importance = [zip_num];\n",
    "    for i in range(len(feature_name)):\n",
    "        mse_feat = [];\n",
    "        for _ in range(TRIALS):\n",
    "            seq_perm = series_transformed.copy();\n",
    "            seq_perm[:, i] = np.random.permutation(seq_perm[:,i])\n",
    "            results, actual = NN_forecast(model, seq_perm, pure = False);\n",
    "            mse_feat.append(mean_squared_error(actual[:-TEST_LENGTH], results[:-TEST_LENGTH])**0.5);\n",
    "\n",
    "        perm_importance.append(sum(mse_feat)/TRIALS/mse - 1.0);\n",
    "        # print(\"{}, {:.4f}\".format(feature_name[i], sum(mse_feat)/TRIALS/mse - 1.0));\n",
    "\n",
    "    collection_importance.append(perm_importance);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e550bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame(collection_importance, columns = [\"zip\"] + feature_name);\n",
    "importance_df.to_csv(\"./Results in CSV files/Importance.csv\", index = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b494627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "_Tensorflow_region_zip_feature_importance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
